{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3\n",
    "\n",
    "\n",
    "class GridWorldEnv:\n",
    "    \"\"\"\n",
    "        Has the following members\n",
    "        - s: the current state\n",
    "        - nA: number of actions\n",
    "        - nS: number of states\n",
    "        - T: transitions, dictionary where\n",
    "                          T[s][a] == (next_state, reward, done)\n",
    "                          done=True means that the episode is over after this step\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.T = {\n",
    "            0: {LEFT: (0, 0, False), RIGHT: (1, 0, False), UP: (0, 0, False), DOWN: (2, 0, False)},\n",
    "            1: {LEFT: (0, 0, False), RIGHT: (1, 0, False), UP: (1, 0, False), DOWN: (3, 0, False)},\n",
    "            2: {LEFT: (2, 0, False), RIGHT: (3, 0, False), UP: (0, 0, False), DOWN: (2, 0, False)},\n",
    "            3: {LEFT: (2, 0, False), RIGHT: (4, 0, False), UP: (1, 0, False), DOWN: (5, 0, False)},\n",
    "            4: {LEFT: (4, -5, True), RIGHT: (4, -5, True), UP: (4, -5, True), DOWN: (4, -5, True)},\n",
    "            5: {LEFT: (5, 0, False), RIGHT: (6, 0, False), UP: (3, 0, False), DOWN: (5, 0, False)},\n",
    "            6: {LEFT: (6, 5, True), RIGHT: (6, 5, True), UP: (6, 5, True), DOWN: (6, 5, True)}\n",
    "        }\n",
    "        self.s = 0\n",
    "        self.nA = 4\n",
    "        self.nS = len(self.T.keys())\n",
    "\n",
    "    def reset(self):\n",
    "        self.s = 0\n",
    "        return self.s\n",
    "\n",
    "    def move(self, a):\n",
    "        next_state, reward, done = self.T[self.s][a]\n",
    "        #self.s = next_state\n",
    "        return next_state, reward, done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q-Learning:\n",
      "there are totally  10 times\n",
      "optimal policy is:  [0 0 0 0 0 0 0] \n",
      "\n",
      "there are totally  37 times\n",
      "optimal policy is:  [2 1 2 1 0 2 0] \n",
      "\n",
      "there are totally  40 times\n",
      "optimal policy is:  [2 1 2 1 0 2 0] \n",
      "\n",
      "there are totally  29 times\n",
      "optimal policy is:  [1 1 2 1 0 2 0] \n",
      "\n",
      "there are totally  32 times\n",
      "optimal policy is:  [2 1 2 1 0 2 0] \n",
      "\n",
      "\n",
      "SARSA:\n",
      "there are totally  1580 times\n",
      "optimal policy is:  [1 0 2 1 0 2 0] \n",
      "\n",
      "there are totally  2028 times\n",
      "optimal policy is:  [1 0 2 1 0 2 0] \n",
      "\n",
      "there are totally  1116 times\n",
      "optimal policy is:  [2 1 3 1 0 2 0] \n",
      "\n",
      "there are totally  1473 times\n",
      "optimal policy is:  [1 0 2 1 0 2 0] \n",
      "\n",
      "there are totally  4194 times\n",
      "optimal policy is:  [2 1 3 1 0 2 0] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "class TD_solver:\n",
    "\n",
    "    def __init__(self, estimator):\n",
    "        self.estimator = estimator # SARSA OR QL\n",
    "        self.grid = GridWorldEnv()\n",
    "        self.q = np.zeros([self.grid.nS, self.grid.nA]) # action-value function array, [states,actions]\n",
    "        self.alpha = 1\n",
    "        self.gamma = 0.9\n",
    "        self.epsilon = 1\n",
    "        self.pi = np.ones([self.grid.nS, self.grid.nA])\n",
    "        \n",
    "    def reset(self):\n",
    "        self.__init__(self.estimator)\n",
    "        \n",
    "    def cal_Pi(self,epsilon):\n",
    "        random = self.epsilon/self.grid.nA\n",
    "        best = 1 - self.epsilon + random\n",
    "\n",
    "        for state in range(self.q.shape[0]):\n",
    "            bestAction = np.argmax(self.q[state]) #choose the best action in every state according to q\n",
    "            self.pi[state][:] = random\n",
    "            self.pi[state][bestAction] = best\n",
    "        \n",
    "    def pickAction(self, s):\n",
    "        a = np.random.choice([i for i in range(self.grid.nA)],\\\n",
    "                            p = self.pi[s])  #pick action randomly with weights.\n",
    "        return a\n",
    "    \n",
    "    def update(self):\n",
    "        self.alpha = self.alpha * 0.9996\n",
    "        self.epsilon = self.epsilon * 0.9992 # decrease epsilon and alpha slowly\n",
    "#         print(self.alpha,\" \",self.epsilon)\n",
    "        \n",
    "    def need_termination(self, q0, q, optP, optP0):\n",
    "#         print(\"\\ndiff(P0,P) \\n=\",(optP0-optP),\"\\n\")\n",
    "#         print(\"\\ndiff(q0,q) \\n=\",(q0-q),\"\\n\")\n",
    "        return np.all(abs(q0-q) < 1e-10) & np.all(abs(optP0-optP) < 1e-10)\n",
    "      \n",
    "    def SARSA_estimator(self, s0, a0):\n",
    "        return self.gamma * self.q[s0,a0] # choose the next action, on-policy\n",
    "    \n",
    "    def QL_estimator(self, s0):\n",
    "        return self.gamma * self.q[s0,np.argmax(self.q[s0])] # don't choose the next action, off-policy\n",
    "    \n",
    "    def solve(self):\n",
    "        self.reset()\n",
    "        Terminated = False\n",
    "        self.cal_Pi(self.epsilon)\n",
    "        q0 = self.q.copy()\n",
    "        count = 0\n",
    "        optP0 = np.array([0 for i in range(7)])\n",
    "        while not(Terminated):\n",
    "            route = []\n",
    "            self.grid.reset()\n",
    "            s0 = self.grid.s\n",
    "            a0 = self.pickAction(self.grid.s)\n",
    "#             print(i,\"th round q =\\n\",self.q)#\n",
    "            done = False\n",
    "            count = count + 1\n",
    "#             print(\"\\n\",i,\"th round: q is\\n\",self.q)\n",
    "           \n",
    "            while not(done):\n",
    "                self.grid.s = s0\n",
    "                route.append(s0)\n",
    "                a = a0\n",
    "#                 print(\"state is \",self.grid.s)#\n",
    "                s0, reward, done = self.grid.move(a)\n",
    "                a0 = self.pickAction(s0)  # for Q-Learning not used\n",
    "                if done:\n",
    "                     self.q[self.grid.s,:] = self.q[self.grid.s,a] + self.alpha*\\\n",
    "                                            (reward - self.q[self.grid.s,a])\n",
    "                elif self.estimator == \"SARSA\":\n",
    "                        self.q[self.grid.s,a] = self.q[self.grid.s,a] + self.alpha*\\\n",
    "                                                (reward + self.SARSA_estimator(s0,a0) - self.q[self.grid.s,a]) \n",
    "                else:\n",
    "                       self.q[self.grid.s,a] = self.q[self.grid.s,a] + self.alpha*\\\n",
    "                                               (reward + self.QL_estimator(s0) - self.q[self.grid.s,a]) \n",
    "#                 if self.q[s0,a0] != 0:\n",
    "#                     print(\"current state is \", self.grid.s)\n",
    "#                     print(\"done is \",done, \n",
    "#                           \"\\nreward is\",reward,\"\\nq[s,a] is\",self.q[self.grid.s,a] )\n",
    "#                     print(self.q)\n",
    "                self.cal_Pi(self.epsilon)\n",
    "    \n",
    "                self.update()\n",
    "#             print(\"afterwards q is\\n\",self.q)\n",
    "            optP =np.array([ np.argmax(self.q[i]) for i in range(self.grid.nS)])\n",
    "            Terminated = self.need_termination(q0,self.q, optP, optP0)\n",
    "            optP0 = optP.copy()\n",
    "            q0 = self.q.copy()  \n",
    "#             print(Terminated)\n",
    "#             print(*route, sep = ' -> ')\n",
    "#             print(self.q)\n",
    "        \n",
    "        print(\"there are totally \", count,\"times\")\n",
    "\n",
    "        return self.pi, self.q, optP\n",
    "\n",
    "    \n",
    "\n",
    "# test\n",
    "e = 5\n",
    "print(\"\\nQ-Learning:\") \n",
    "s1 = TD_solver(\"QL\")\n",
    "for i in range(e):\n",
    "#     print(\"state-value function:\\n \",s1.solve()[0])\n",
    "#     print(\"action-value function: \\n\",s1.solve()[1])\n",
    "    print(\"optimal policy is: \",s1.solve()[2],\"\\n\")\n",
    "    \n",
    "print(\"\\nSARSA:\")\n",
    "s2 = TD_solver(\"SARSA\")\n",
    "for i in range(e):\n",
    "#     print(\"state-value function: \\n \",s2.solve()[0])\n",
    "#     print(\"action-value function: \\n\",s2.solve()[1])\n",
    "    print(\"optimal policy is: \",s2.solve()[2],\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
